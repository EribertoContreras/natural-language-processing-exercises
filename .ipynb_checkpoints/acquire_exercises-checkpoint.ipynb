{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f68f763",
   "metadata": {},
   "source": [
    "By the end of this exercise, you should have a file named acquire.py that contains the specified functions. If you wish, you may break your work into separate files for each website (e.g. acquire_codeup_blog.py and acquire_news_articles.py), but the end function should be present in acquire.py (that is, acquire.py should import get_blog_articles from the acquire_codeup_blog module.)\n",
    "\n",
    "1) Codeup Blog Articles\n",
    "\n",
    "- Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "- Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "\n",
    "\n",
    "`{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}`\n",
    "\n",
    "- Plus any additional properties you think might be helpful.\n",
    "\n",
    "`Bonus:`  Scrape the text of all the articles linked on codeup's blog page.\n",
    "\n",
    "2) News Articles\n",
    "\n",
    "- We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "- Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "    - Business\n",
    "    - Sports\n",
    "    - Technology\n",
    "    - Entertainment\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:\n",
    "\n",
    "\n",
    "`{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}`\n",
    "\n",
    "- `Hints:`\n",
    "\n",
    "- Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "- Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "- Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "- Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any additional processing that needs to be done.\n",
    "\n",
    "3) `Bonus:` cache the data\n",
    "\n",
    "Write your code such that the acquired data is saved locally in some form or fashion. Your functions that retrieve the data should prefer to read the local data instead of having to make all the requests everytime the function is called. Include a boolean flag in the functions to allow the data to be acquired \"fresh\" from the actual sources (re-writing your local cache)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f3141",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ef351",
   "metadata": {},
   "source": [
    "\n",
    "1) Codeup Blog Articles\n",
    "\n",
    "- Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "- Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "\n",
    "\n",
    "`{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}`\n",
    "\n",
    "- Plus any additional properties you think might be helpful.\n",
    "\n",
    "`Bonus:`  Scrape the text of all the articles linked on codeup's blog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b121b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ed057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_articles():\n",
    "    def in_person_workshop_data():\n",
    "        url = \"https://codeup.com/workshops/in-person-workshop-learn-to-code-python-on-7-19/\"\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        response.text\n",
    "        # Make a soup variable holding the response content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return (soup.title, soup.contents)\n",
    "    def free_javascript_data():\n",
    "        url = \"https://codeup.com/workshops/dallas/free-javascript-workshop-at-codeup-dallas-on-6-28/\"\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        response.text\n",
    "        # Make a soup variable holding the response content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return (soup.title, soup.contents)\n",
    "    def tips_for_prospective_students():\n",
    "        url = \"https://codeup.com/tips-for-prospective-students/is-our-cloud-administration-program-right-for-you/\"\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        response.text\n",
    "        # Make a soup variable holding the response content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return (soup.title, soup.contents)\n",
    "    def pride_in_tech_panel():\n",
    "        url = \"https://codeup.com/workshops/pride-in-tech-panel/\"\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        response.text\n",
    "        # Make a soup variable holding the response content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return (soup.title, soup.contents)\n",
    "    def tips_for_prospective_students():\n",
    "        url = \"https://codeup.com/tips-for-prospective-students/mental-health-first-aid-training/\"\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        response.text\n",
    "        # Make a soup variable holding the response content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return (soup.title, soup.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63414d8e",
   "metadata": {},
   "source": [
    "***\n",
    "2) News Articles\n",
    "\n",
    "- We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "- Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "    - Business\n",
    "    - Sports\n",
    "    - Technology\n",
    "    - Entertainment\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:\n",
    "\n",
    "\n",
    "`{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}`\n",
    "\n",
    "- `Hints:`\n",
    "\n",
    "- Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "- Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "- Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "- Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any additional processing that needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c45189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bb46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec645b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
